---
title: "EP1 - Applied Macroeconometrics"
author: "Monitor: Matheus Carrijo de Brito"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
       highlight: textmate
       theme: flatly
       number_sections: true
       code_folding: show
       toc: true
       toc_float: false

  #html_document:
  #     highlight: textmate
  #     theme: flatly
  #     number_sections: true
  #     code_folding: show
  #     toc: true
  #     toc_float: false
bibliography: references.bib
---

### Applied part

```{r warning=FALSE, include=FALSE, fig.height=7, fig.width=12, fig.align="center"}
rm(list = ls()) 

library(data.table) # Data manipulation
library(dynlm)      # Dynamic Linear Non Linear Models 
library(vars)       # Var pagkage, in thi
library(ggplot2)    # Plots 
library(svars)      # Structural Vars estimations 
library(tstools)
library(kableExtra)
library(Quandl)
library(tseries)
library(latex2exp)
library(dplyr)
library(stargazer)

```


```{r}

tickers_names <- c("GNPC96", "UNRATE")

quantmod::getSymbols.FRED(Symbols = tickers_names, env = .GlobalEnv, return.class = "xts")

# Other way to download 
#gnp_raw <- Quandl("FRED/GNPC96", type = 'xts')
#u_raw <- Quandl("FRED/UNRATE", type = 'xts') 

```

```{r, fig.align="center", fig.width = 10, fig.height=4}

# Plotting the product data 

ggplot(data = GNPC96) + 
  geom_line(aes(x = Index, y = GNPC96)) + 
  scale_x_date(breaks = "5 years", date_labels = "%Y") + 
  scale_y_continuous(breaks = seq(2500, 20000, by = 5000)) + 
  labs(x = "Date", y = "GNP", title = "GNP Time Series") + 
  theme_light()


```

```{r,  fig.align="center", fig.width = 10, fig.height=4}

# Plotting the unemployment data

ggplot(data = UNRATE) + 
  geom_line(aes(x = Index, y = UNRATE)) + 
  scale_x_date(breaks = "5 years", date_labels = "%Y") + 
  scale_y_continuous(breaks = seq(2.5, 15, by = 2.5)) + 
  labs(x = "Date", y = "Rate", title = "Unemployment Time Series") + 
  theme_light()



```

```{r, warning=FALSE}

# Note that are required some transformation in data since unemployment is at monthly frequency and the GNP is on the level.  

# Taking the difference in GNP  

dgnp <- 100*diff(log(GNPC96), lag = 1) |> na.omit() 

# Turning the data unemployment data quartely 

u_quartely <- apply.quarterly(x = UNRATE, FUN = mean)
  
index(u_quartely) <- as.Date(as.yearqtr(index(u_quartely)))
  
# Subsetting the window of Blanchard and Quah (1989) 

data_set <- merge(dgnp, u_quartely, join = "inner")["1948-04-01/1987-10-01"] |> 
  as.zoo()


```

In the article, the authors arguments that after oil shock the growth of GNP becomes lower. Indeed, a simple descriptive analysis shows this effect. Before the shock, the annualized growth was 3.9 and after was 2.8. Consistent with the oil shock being a supply shock, the unemployment also increased in the same period. The Average unemployment was 4.7 % and growth to 7.3 % after the shock. 

```{r}

# Making the column with the periods 
data_set$period <- fcase(dplyr::between(index(data_set), 
                                 left =  as.Date("1948-04-01"),
                                 right = as.Date("1973-07-01")), 1, 
                         dplyr::between(index(data_set), 
                                 left =  as.Date("1973-10-01"),
                                 right = as.Date("1987-01-01")), 2)
   

# Making a descriptive table 

data_set |> 
  as.data.frame() |> 
  na.omit() |>
  group_by(period) |> 
  summarise(`Annualized Mean Growth` = mean(GNPC96)*4, 
            `Unemployment Rate` = mean(UNRATE)) |> 
  mutate(period = c("Before First Oil Crash", "After First Oil Crash")) |>
  kbl(digits = 3, title = "Difference between periods",     
      col.names = c("Period", "Annualized Mean Growth (%)", "Unemployment Rate (%)"), 
      booktabs = T) |>
  kable_styling() 

```

To assure stationarity of all variables in our VAR, we need to extract the cycle component of each series the authors employs two transformation in the original series.

-   **Unemployment:** remove an estimated linear trend (OLS regression)

-   **GNP Growth:** remove level shift due to lower growth after 1974 (OLS regression).

We will proceed in the same way. Also, we included the constant coefficient to get a demean series.

```{r, warning=FALSE, fig.align="center", fig.width=10, fig.height=7}

detrend_u <- dynlm(data_set$UNRATE ~ trend(data_set$UNRATE))

# Extract break in GNP data (lower growth rate after 1974)

dummy <- zoo(tstools::create_dummy_ts(end = c(1987,4), 
                                  dummy_start = c(1974,1), 
                                  dummy_end = c(1987,4), 
                                  start_basic  = c(1948,2), 
                                  basic_value = 0, 
                                  dummy_value = 1,
                                  frequency = 4), order.by = index(data_set))

# Merging all data  
data_set <- merge(data_set, dummy)

# Estimation of demean gnp 
demean_gnp <- dynlm(data_set$GNPC96 ~ 1 + dummy, data = data_set)

# Extracting the residuals 
data_set$gnp <- demean_gnp$residuals 

data_set$u <- detrend_u$residuals 

# Plotting the data 

data_set |> 
  data.frame(date = index(data_set)) |> 
  dplyr::select(-c("dummy", "period")) |> 
  data.table::melt(measure.vars = c(1,2,3,4), variable.name = "variable", value.name = "value") |>
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  facet_wrap( ~ variable, ncol = 2, scale = "free",
            labeller = as_labeller(c(GNPC96 = "Growth Rate Output Original (%)", 
                                     UNRATE = "Unemployment Rate Original (%)",
                    gnp = "Growth Rate Output Cycle (%)", 
                    u = "Unemployment Rate Cycle (%)"))) + 
  labs(x = 'Quarter', y = 'Percentage') + 
  theme_bw() + 
  scale_x_date(breaks = "5 years", date_labels = "%Y")
  
```

# Var estimation

##  Information criteria:

In general the estimation of VAR model requires the estimation of many parameters. For example, in a VAR(p) with k  variables there are $k + k^{2}p + K(K-1)/2$ parameters to estimate. The main problem is in time-series analyses we often deal with short-periods of time. Thus, the estimation of many parameters implies in a trade-off since the addition of lags increase the information in the model. However, more imprecisely is the estimation of those parameters. Another problems that may arise is the **overfitting**. The model is great in sample but perform poorly in forecasting. 

Information Criteria are metrics used to choose the right number of lags in the specification of a VAR model. The general formula is:

$$
CI(I) =  ln(\hat{\Sigma}) + c_{T}\Psi(I)
$$
where $c_{T}$ is a function that penalize the inclusion of lags and $\Psi(I)$ is the number of parameters estimated using the I lags. The most common information criteria are:

- Akaike information: $C_{T} = \frac{2}{T}$ 

- Schwarz Information or Bayesian Information (BIC) $c_{T} = \frac{log(T)}{T}$ 

- Hannah-Quinn Information: $c_{T} = \frac{2log(log(T))}{T}$ 

Note that for a fair comparison between the models is necessary to use the same sample and the same T. 

Is possible to demonstrate that for $T \geq 16$

$$
\hat{P}_{B I C} \leq \hat{p}_{H Q} \leq \hat{p}_{A I C}
$$

Moreover, BIC and HQ are consistent estimators of the true number of lags which does not hold for the AIC information. However, the advantage of AIC is the better properties in finite sample. More precisely, the lag choosen by AIC is more closer to the true lag. 

Another important criteria used to choose the right number of lags is checking with the residuals of the model are like white-noise. That is, without serial-correlation. One test to check this is the Portmanteu, LM tests. 


```{r}

information_criteria <- c("AIC", "SC", "HQ")

var_models <- list(var_aic  = NA, var_sc = NA, var_hq = NA)

for(i in seq_along(var_models)){
  
  var_models[[i]] <- VAR(y = data_set[, c("gnp","u")], lag.max = 20, 
                         ic = information_criteria[i])
  
}

# Example of LM test 
#arch.test(var_models$var_aic, lags.multi = 8, multivariate.only = T)

# Example of Portmanteu test 
#serial.test(x = var_models$var_aic, lags.pt = 8, type = "PT.adjusted")

# Table with the Optimal Lag using each Information Criteria 
kbl(VARselect(data_set[, c("gnp","u")], lag.max = 20)$selection, 
    caption = "Optimal number of Lags by Information Criteria", 
    col.names = "Lags", align = "c", booktabs = T) |> 
  kable_styling()

```

# Printing the results of the estimations

```{r,  results ='asis', message = FALSE, echo=TRUE}

var8 <- VAR(y = data_set[, c("gnp","u")], p =8, type = "none")

stargazer::stargazer(var8$varresult$gnp,
                     var8$varresult$u, 
                     type = "html",  
                     dep.var.labels = c("Variation GNP", "Unemployment"), 
                     dep.var.labels.include = T)

```

# Identification

We have learned until now two ways of identification. The first one is imposing restriction on the contemporaneous effect between variables by applying restriction on the Cholesky decomposition matrix. The second methods imposes restriction on the long-term relation between the variables, this is the method proposed by [@blanchard1988]. We will proceed this exercise analyzing both ways of identification.

The most general form of a VAR is writting in the structural form given by: 

$$
A_{0} y_{t}=b+\sum_{j=1}^{p} A_{j} y_{t-j} + Bu_{t}
$$

The **short term restriction** is imposing some structure in the contemporaneous effects between the variables which implies that some coefficient in the matrix $A_{0}$ is imposed. 


```{r}

# Note that the order matters using short-run identification !!! 

svar_1 <- VAR(y = data_set[, c(5, 6)], p = 8, type =  "none") |> id.chol() 

svar_2 <- VAR(y = data_set[, c(6, 5)], p = 8, type =  "none") |> id.chol()

```


## IRF function  

This function allows us to know how the variables responds to structural shocks. Which is mathematically expressed in the derivative

$$
\operatorname{IRF}_{j}(h) \equiv \frac{\partial y_{t+h}}{\partial u_{j, t}}
$$
Note that since every VAR(p) have a representation as VAR(1) we can use the simplified notation:

$$
Y_{t} = c + \Phi_{1}Y_{t-1} + Bu_{} 
$$
Thus, the IRF can be written as $IRF_{j} = \Phi^{h}_{1}Bu_{j}$. 

## FEVD 

The decomposition of forecast error gives information about how relevant are each structural shock in explaining the error in forecast. Using the VMA formulation in the reduced form we can visualize the MSE which is given by the following equations. 

$$
\begin{aligned}
\tilde{y}_{t+h \mid t} & \equiv y_{t+h}-y_{t+h \mid t}=\sum_{j=0}^{h-1} \Phi_{1}^{j} \varepsilon_{t+h-j} \\
\operatorname{MSE}\left(\tilde{y}_{t+h \mid t}\right) &=\sum_{j=0}^{h-1} \Phi_{1}^{j} E\left[\varepsilon_{t+h-j} \varepsilon_{t+h-j}^{\prime}\right]\left(\Phi_{1}^{j}\right)^{\prime}
\end{aligned}
$$
using that the relation between residual and structural shock, $e_{t} = B u_{t}$

$$
MSE\left(\tilde{y}_{t+h \mid t}\right) = \sum_{j=0}^{h-1} = \Phi^{j} B B^{\prime}\left(\Phi^{j}\right)^{\prime}
$$ 


```{r, fig.align='center', results='hold'}

plot(mb.boot(svar_1, n.ahead = 40, nboot = 1000), lowerq = 0.16, upperq = 0.84, distr = "gaussian") + 
  theme_bw(base_size = 12) + 
  labs(title = 'IRFs: no contemporanous effect of u on dgnp')

plot(mb.boot(svar_2, n.ahead = 40, nboot = 1000), lowerq = 0.16, upperq = 0.84, distr = "gaussian") + 
  theme_bw(base_size = 12) + 
  labs(title = 'IRFs: no contemporanous effect of gnp on u')
                                 
```

##  Long term restriction

We interpret the disturbances that have a **temporary effect** on output as being mostly **demand disturbances**, and those that have a **permanent effect** on output as mostly **supply disturbances**. Note that in order to impose the long-run restriction to identification is used lower-triangular matrix obtained via Choleski decomposition. Therefore, the order of contemporaneous effect matters in the identification.

The long run restriction used by [@citeBlanchard1988] can be expressed in the following way. Let $Y = (\Delta GNP, U)$in to simplification we can write the model as a SVAR(1). 


$$
Y_{t} = c + \Phi_{1}Y_{t-1} + Bu_{t}
$$
We can write the model in the VMA formula 

$$
Y_{t} = c(I - \Phi_{1})^{-1} + (I - \Phi_{1})^{-1}Bu_{t}
$$
with we denote $D = (I - \Phi_{1})^{-1}B$ there assumption is that 

$$
\left(\begin{array}{c}
\Delta G D P_{t, \infty} \\
u_{t, \infty}
\end{array}\right)=\underbrace{\left(\begin{array}{cc}
D_{11} & 0 \\
D_{21} & D_{22}
\end{array}\right)}_{\equiv D}\left(\begin{array}{c}
u_{t}^{s} \\
u_{t}^{d}
\end{array}\right)
$$

- Supply shock have long-run impact on gdp and unemployment
- Demand Shock have long-run impact only in the unemployment, thus gdp growth is not affect in the long-run by demand shock. 

```{r ,warning = F}

var_bq <- VAR(y = data_set[, c("gnp", "u")], p = 8, type = "none")

svar_bq <- id.chol(var_bq)

bq_ident <- BQ(var_bq)

summary(bq_ident)

```

```{r , warning=F}

# Construction of some useful functions to make plotting easier 

irf_bq <- vars::irf(bq_ident, n.ahead = 40, boot = T, ci = 0.68)

plot(irf_bq)

#irf_bq_cum <- vars::irf(bq_ident, n.ahead = 40, boot = T, ci = 0.68, cumulative = T)

extract_df_irf <- function(model, response, impulse, cumulative){
  
  irf <- vars::irf(model, n.ahead = 40, boot = T, ci = 0.68,
                   impulse = impulse, response = response, cumulative = cumulative)
  
  df_irf <- data.frame(irf[["irf"]][[impulse]],
              irf[["Lower"]][[impulse]], 
              irf[["Upper"]][[impulse]], 
              periods = 1:dim(irf[["irf"]][[1]])[1]) |> 
    setNames(c("response", "response_lower", "response_upper", "periods")) 
  
  return(df_irf)
}


plot_irf <- function(model, impulse, response, cumulative, title){

  irf_plot <- ggplot(data = extract_df_irf(model = model, response = response, impulse = impulse, cumulative=cumulative)) +
  geom_line(aes(x = periods, 
               y = response))  +
    geom_hline(yintercept = 0, color="red") +
    geom_ribbon(aes(x = periods, 
               y = response, ymin = response_lower, ymax = response_upper), 
               fill = "grey", alpha = .4, 
               color = "grey50", linetype = "dashed") +
    labs(x = "Time", y = "Response") + 
    theme_light() +
    ggtitle(title) +
    theme(plot.title = element_text(size = 11, hjust=0.5),
          axis.title.y = element_text(size=11))

  return(irf_plot)
}


```

### Making and plotting the IRFs

```{r}

irf_gnp_shock_gnp <- plot_irf(model = bq_ident, impulse = "gnp", response = "gnp", cumulative = T, 
                    title = TeX("Supply Shock $\\rightarrow$ Response Output$"))

irf_gnp_shock_unem <- plot_irf(model = bq_ident, impulse = "u", response = "gnp", cumulative = T, 
                    title = TeX("Demand Shock $\\rightarrow$ Response Output"))

irf_unem_shock_dgnp <- plot_irf(model = bq_ident, impulse = "gnp", response = "u", cumulative = T,  
                    title = TeX("Supply Shock $\\rightarrow$ Response Unemployment"))

irf_unem_shock_unem <- plot_irf(model = bq_ident, impulse = "u", response = "u", cumulative = F, 
                    title = TeX("Demand Shock $\\rightarrow$ Response Unemployment"))


# All plots together 

gridExtra::grid.arrange(irf_gnp_shock_gnp, irf_gnp_shock_unem, 
                        irf_unem_shock_dgnp, irf_unem_shock_unem) 


```

### Main findings

\- Demand disturbances have a hump-shaped effect on both output and unemployment;

\- The effect peaks after a year and vanishes after two to three years;

\- Up to a scale factor, the dynamic effect on unemployment of demand disturbances is a mirror image of that on output;

\- The effect of supply disturbances on output increases steadily over time, to reach a peak after two years and a plateau after five years;

\- Favorable supply disturbances may initially increase unemployment. This is followed by a decline in unemployment, with a slow return over time to its original value;

# Forecast Error Variance Decomposition (FEVD)

In this section we analyse the contribution of each shock to explain the variance of the forecast.


```{r, fig.align="center", fig.height=7}

plot(fevd(x = bq_ident, n.ahead = 40))

```


# Construction of the counter-factual series

Note that in the estimation of reduced form we have residual that related with both structural shock. Since every residual series is a linear combination of structural shock. Thus, an interesting exercise decompose the predict value of y by fixing one structural shock. In [@citeBlanchard1988] make this exercise in the figures 7 to 9. For example, the authors fixed the supply shock and analyse the impact of demand shock in the variation of gnp growth. We will make a similar exercise in the following codes. 


```{r, warning=F}

N = 8

# Matrix-Array of the coefficient in the VMA formulation
phi <-  Phi(x = var_bq, nstep = N)

# Matrix with the residual from var estimation using the identification based on 
# blanchard and quah

resid_var_bq <- residuals(var_bq)

# Predicting the [gnp, u]^{\prime}
predict_y_demand <- data.frame(t(matrix(NA, 
                    nrow = 2, 
                    ncol = length(var_bq$varresult$gnp$fitted.values))))

predict_y_supply <- data.frame(t(matrix(NA, 
                    nrow = 2, 
                    ncol = length(var_bq$varresult$gnp$fitted.values))))

# Upper 1 to supply shock, Lower 1 for demand shock

matrix_shock_supply <- matrix(c(1,0,0,0), ncol=2, nrow =2)

matrix_shock_demand <- matrix(c(0,0,0,1), ncol=2, nrow =2)

# Recursive formula 

for(t in 9:length(var_bq$varresult$gnp$fitted.values)){

predict_y_supply_aux = matrix(NA, nrow = 2, ncol = N)

predict_y_demand_aux = matrix(NA, nrow = 2, ncol = N)

for(i in seq_len(N)){
  
  predict_y_demand_aux[, i]= phi[, , i] %*%  
    solve(bq_ident$B) %*% bq_ident$A %*% t(resid_var_bq[t-(i-1),] %*% matrix_shock_demand)
  
  predict_y_supply_aux[, i]= phi[, , i]  %*% 
    solve(bq_ident$B) %*% bq_ident$A %*% t(resid_var_bq[t-(i-1), ] %*% matrix_shock_supply)
  
}

predict_y_demand[t, 1] <- sum(predict_y_demand_aux[1, ])

predict_y_demand[t, 2] <- sum(predict_y_demand_aux[2, ])

predict_y_supply[t, 1] <- sum(predict_y_supply_aux[1, ])

predict_y_supply[t, 2] <- sum(predict_y_supply_aux[2, ])

}

# Data Set 
data_set_cf <- merge(data_set[, c("gnp", "u")], 
      xts(predict_y_demand, order.by = index(data_set)[9:159]) , 
      xts(predict_y_supply, order.by = index(data_set)[9:159])) |> 
  data.frame() |>
  mutate(date = index(data_set[, c("gnp", "u")])) |>
  setNames(c("gnp", "u", "cf_gnp_demand_shock", "cf_u_demand_shock", 
             "cf_gnp_supply_shock", "cf_u_supply_shock", "date"))

```

# Plotting the counter-factual series for output and unemployement

```{r, warning=F, fig.align="center", fig.width=10, fig.height=7}
# Plot Output Decomposition 
data_set_cf |>
  select(c("date",  "gnp","cf_gnp_demand_shock",  "cf_gnp_supply_shock")) |>
  na.omit() |>
  melt(id.vars = "date", variables.names = "variable", value.name = "value") |>
  ggplot() + 
  geom_line(aes(x = date, y = value)) + 
  facet_wrap(~variable, ncol = 1, nrow = 3, scale = "free", 
             labeller = as_labeller(c(gnp = "Output Flutuations", 
                                    cf_gnp_demand_shock = "Output Flutuations due to Demand Shock", cf_gnp_supply_shock = "Output Flutuations due to Suppply Shock"))) + 
  theme_grey() + 
  scale_x_date(date_labels = "%Y", breaks = "2.5 years")

  
# Plots Unemployment decomposition 

data_set_cf |>
  select(c("date","u", "cf_u_demand_shock", "cf_u_supply_shock")) |>
  na.omit() |>
  melt(id.vars = "date", variables.names = "variable", value.name = "value") |>
  ggplot() + 
  geom_line(aes(x = date, y = value)) + 
  facet_wrap(~variable, ncol = 1, nrow = 3, scale = "free",  
             labeller = as_labeller(c(u = "Unemployment Flutuations", 
                                    cf_u_demand_shock = "Unemployment Flutuations due to Demand Shock", cf_u_supply_shock = "Unemployment Flutuations due to Supply Shock"))) + 
  theme_grey() + 
  scale_x_date(date_labels = "%Y", breaks = "2.5 years")

```

# Plotting the counter-factual series for output flutuations in level

```{r warning=F, fig.align="center", fig.height=7, fig.width=10}

data_set_cf |>
  select(c("date",  "gnp","cf_gnp_demand_shock",  "cf_gnp_supply_shock")) |>
  na.omit() |>
  mutate(gnp = 100*(1+cumsum(gnp)/100), 
         cf_gnp_demand_shock = 100*(1+cumsum(cf_gnp_demand_shock)/100), 
         cf_gnp_supply_shock = 100*(1+cumsum(cf_gnp_supply_shock)/100)) |> 
  melt(id.vars = "date", variable.names = "variable", value.name = "value") |>
  ggplot() + 
  geom_line(aes(x = date, y = value)) + 
  facet_wrap(~variable, ncol = 1, nrow = 3, 
             labeller = as_labeller(c(gnp = "Output Flutuations", 
                                    cf_gnp_demand_shock = "Output Flutuations due to Demand Shock", cf_gnp_supply_shock = "Output Flutuations due to Suppply Shock"))) + 
  theme_grey() + 
  scale_x_date(date_labels = "%Y", breaks = "2.5 years")

```

### Now we need to use the full sample

```{r, warning=F}

dt_full <- merge(x = dgnp, u_quartely, join = "inner")["1948-07-01/2021-04-01"] |> as.zoo()

names(dt_full) <- c("gnp", "u")

# Making the transformations 

dt_full$dummy <- zoo(tstools::create_dummy_ts(end = c(2021,1), 
                                  dummy_start = c(1974,1), 
                                  dummy_end = c(2021,1), 
                                  start_basic = c(1948,2), 
                                  basic_value = 0, 
                                  dummy_value = 1,
                                  frequency = 4), 
                     order.by = index(dt_full))

# Creating a dummy 

# Making the unemployment residuals 
dt_full$u_2 <- dynlm(dt_full$u ~ 1 + trend(dt_full$u), data = dt_full) |> residuals()

# Making the structural break on GNP  

dt_full$gnp_2 <- dynlm(dt_full$gnp ~ 1 + dummy, data = dt_full) |> residuals()

```

# Printing the results from VAR using the full sample

```{r results ='asis', message = FALSE, echo=FALSE}

var_full <- VAR(dt_full[, c(1,2)], p = 8, type = "const") 

stargazer::stargazer(var_full$varresult$gnp, 
                     var_full$varresult$u, 
                     type = "html", 
                     title = "Unrestricted VAR results", style = "aer")
```

# Using the full sample 

```{r, warning = F}

bq_ident_full = BQ(var_full)


summary(bq_ident)
summary(bq_ident_full)


# Non-cumulative and cumulative IRFs

irf_gnp_shock_gnp_full <- plot_irf(model = bq_ident_full, impulse = "gnp", response = "gnp", cumulative = T, 
                    title = TeX("Supply Shock $\\rightarrow$ Response Output$"))

irf_gnp_shock_unem_full <- plot_irf(model = bq_ident_full, impulse = "u", response = "gnp", cumulative = T, 
                    title = TeX("Demand Shock $\\rightarrow$ Response Output"))

irf_unem_shock_dgnp_full <- plot_irf(model = bq_ident_full, impulse = "gnp", response = "u", cumulative = F,  
                    title = TeX("Supply Shock $\\rightarrow$ Response Unemployment"))

irf_unem_shock_unem_full <- plot_irf(model = bq_ident_full, impulse = "u", response = "u", cumulative = F, 
                    title = TeX("Demand Shock $\\rightarrow$ Response Unemployment"))


gridExtra::grid.arrange(irf_gnp_shock_gnp_full, irf_gnp_shock_unem_full,
                        irf_unem_shock_dgnp_full, irf_unem_shock_unem_full)

```